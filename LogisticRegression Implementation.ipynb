{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:40.368913Z",
     "start_time": "2018-11-12T21:14:39.970765Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:g}'.format(x))\n",
    "np.set_printoptions(suppress=True)\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "\n",
    "def stablesoftmax(x):\n",
    "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x < 0:\n",
    "        a = np.exp(x) \n",
    "        return a / (1 + a) \n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:49.354033Z",
     "start_time": "2018-11-12T21:14:49.324043Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def stablesoftmax(x):\n",
    "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x < 0:\n",
    "        a = np.exp(x) \n",
    "        return a / (1 + a) \n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class LogRegressor():\n",
    "    def __init__(self, tags):  \n",
    "        \"\"\"LogRegressor class constructor\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        tags: list of string\n",
    "        \"\"\"\n",
    "        self.__version__ = 'v0.3'\n",
    "        # `set` will drop duplicated tags\n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        # A dictionary that contains the mapping of sentence words and tags into indexes (to save memory)\n",
    "        # example: self._vocab ['exception'] = 17 means that the word \"exception\" has an index of 17\n",
    "        self._vocab = {} #defaultdict(lambda: len(self._vocab))\n",
    "        \n",
    "        # parameters of the model: weights\n",
    "        # for each class / tag we need to store its own vector of weights\n",
    "        # By default, all weights will be zero\n",
    "        # we do not know in advance how many scales we will need\n",
    "        # so for each class we create a dictionary of a variable size with a default value of 0\n",
    "        # example: self._w['java'][self._vocab['exception']] contains weight for word exception and tag java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # parameters of the model: bias term or w_0 weight\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        \"\"\"Update vocab with new words from words_list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        words_list: list of strings\n",
    "        \"\"\"\n",
    "        for word in words_list:\n",
    "            # every new word will get index=len(self._vocab)\n",
    "            # so at the end of training all wards will numbered from 0 to len(self._vocab)\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        \"\"\"Build words vocab from dataframe column of lists\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas.Dataframe\n",
    "        \n",
    "        column_name: string\n",
    "        \"\"\"\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        \"\"\"Fit single sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: pandas.Series\n",
    "            dict-like object which contains qeustion and his tags\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Series object with metrics for sample\n",
    "        \"\"\"\n",
    "        # sample.name is value from df.index aka row number\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        \n",
    "        sample_loss = 0\n",
    "        \n",
    "        # derive the gradients for each tag\n",
    "        for tag in self._tags:\n",
    "            # target is 1 if current emample has current tag \n",
    "            y = int(tag in tags)\n",
    "            # calculate linear combination of weights and features\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "            z = self._b[tag]\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                # in the test mode, ignore the words that are not in the vocabulary\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "                z += self._w[tag][self._vocab[word]]\n",
    "                \n",
    "            # calculate the probability of tag \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            sigma = sigmoid(z)\n",
    "\n",
    "            # update the value of the loss function for the current example\n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "            sample_loss -= y*np.log(max(sigma,self.tolerance)) + (1 - y) * np.log(1 - min(sigma,1-self.tolerance))\n",
    "\n",
    "            # If still in the training part, update the parameters\n",
    "            if sample_id < self.top_n_train:\n",
    "                # compute the log-likelihood derivative by weight\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "                dLdw = y - sigma\n",
    "                \n",
    "                # make gradient descent step\n",
    "                # We minimize negative log-likelihood (second minus sign)\n",
    "                # so we go to the opposite direction of the gradient to minimize it (the first minus sign)\n",
    "                delta = self.learning_rate * dLdw\n",
    "                for word in (question):                        \n",
    "                    self._w[tag][self._vocab[word]] -= -delta\n",
    "                self._b[tag] -= -delta\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        return pd.Series({'loss': sample_loss})\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16):\n",
    "        \"\"\"One run through dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            pandas DataFrame with question and tags data\n",
    "\n",
    "        top_n_train : int\n",
    "            first top_n_train samples will be used for training, the rest are for the test\n",
    "            default=60000\n",
    "\n",
    "        learning_rate : float \n",
    "            gradient descent training speed\n",
    "            default=0.1\n",
    "\n",
    "        tolerance : float \n",
    "            used for bounding the values of logarithm argument\n",
    "            default=1e-16\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame with metrics for each sample\n",
    "        \"\"\"\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        # generating self._vocab\n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        # Show progress every self.show_period sample, 1% by default\n",
    "        self.show_period = self.total_len // 100\n",
    "        # apply self.fit_sample to each row (sample) of dataframe\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.102197Z",
     "start_time": "2018-11-12T21:19:34.085202Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.5'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        for word in words_list:\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        sample_loss = 0\n",
    "        predicted_tags = None\n",
    "\n",
    "        for tag in self._tags:\n",
    "            y = int(tag in tags)\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "            z = self._b[tag]\n",
    "\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "                z += self._w[tag][self._vocab[word]]\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            sigma = sigmoid(z)\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "            sample_loss -= y*np.log(max(sigma,self.tolerance)) + (1 - y) * np.log(1 - min(sigma,1-self.tolerance))\n",
    "\n",
    "            if sample_id < self.top_n_train:\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "                dLdw = y - sigma\n",
    "\n",
    "                delta = self.learning_rate*dLdw\n",
    "                for word in question:\n",
    "                    # HERE'S YOUR CODE\n",
    "                    # self._w[tag][self._vocab[word]] -= (- delta...\n",
    "                    self._w[tag][self._vocab[word]] -= (-delta +  self.learning_rate * \n",
    "                                                        self.lambda_ * self._w[tag][self._vocab[word]])\n",
    "                #Regulzrizaton - once for every unique entry\n",
    "#                 for word in set(question):\n",
    "#                     self._w[tag][self._vocab[word]] -= \n",
    "                self._b[tag] -= -delta\n",
    "            else:\n",
    "                if predicted_tags is None:\n",
    "                    predicted_tags = []\n",
    "                # HERE'S YOUR CODE\n",
    "                # if sigma... :\n",
    "                if sigma > self.accuracy_level:\n",
    "                    predicted_tags.append(tag)\n",
    "                #     predicted_tags...\n",
    "\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        if predicted_tags is not None:\n",
    "            # HERE'S YOUR CODE\n",
    "            Jaccard = float(len(set(tags).intersection(set(predicted_tags))))/(len(tags) + len(predicted_tags) - \n",
    "                                                                     len(set(tags).intersection(set(predicted_tags))))\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': Jaccard})\n",
    "        else:\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': np.NaN})\n",
    "\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16,\n",
    "                      accuracy_level=0.9,\n",
    "                      lambda_=0.01):\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.accuracy_level = accuracy_level\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        self.show_period = self.total_len // 100\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:39:10.815508Z",
     "start_time": "2018-11-12T21:39:10.797514Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.7'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        for word in words_list:\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        sample_loss = 0\n",
    "        predicted_tags = None\n",
    "\n",
    "        for tag in self._tags:\n",
    "            y = int(tag in tags)\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "            z = self._b[tag]\n",
    "\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "                z += self._w[tag][self._vocab[word]]\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            sigma = sigmoid(z)\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "            sample_loss -= y*np.log(max(sigma,self.tolerance)) + (1 - y) * np.log(1 - min(sigma,1-self.tolerance))\n",
    "\n",
    "            if sample_id < self.top_n_train:\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "                dLdw = y - sigma\n",
    "\n",
    "                delta = self.learning_rate*dLdw\n",
    "                for word in question:\n",
    "                    # HERE'S YOUR CODE\n",
    "                    # self._w[tag][self._vocab[word]] -= (- delta...\n",
    "                    weight = self._w[tag][self._vocab[word]]\n",
    "                    regularization = self.learning_rate * self.lambda_ * (2 * self.gamma * weight + (1 - self.gamma) * np.sign(weight))\n",
    "                    self._w[tag][self._vocab[word]] -= (-delta + regularization)\n",
    "                self._b[tag] -= -delta\n",
    "            else:\n",
    "                if predicted_tags is None:\n",
    "                    predicted_tags = []\n",
    "                # HERE'S YOUR CODE\n",
    "                # if sigma... :\n",
    "                if sigma > self.accuracy_level:\n",
    "                    predicted_tags.append(tag)\n",
    "                #     predicted_tags...\n",
    "\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        if predicted_tags is not None:\n",
    "            # HERE'S YOUR CODE\n",
    "            # Jaccard = ...\n",
    "            Jaccard = float(len(set(tags).intersection(set(predicted_tags))))/(len(tags) + len(predicted_tags) - \n",
    "                                                                     len(set(tags).intersection(set(predicted_tags))))\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': Jaccard})\n",
    "        else:\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': np.NaN})\n",
    "\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16,\n",
    "                      accuracy_level=0.9,\n",
    "                      lambda_=0.001,\n",
    "                      gamma = 0.1):\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.accuracy_level = accuracy_level\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        self.show_period = self.total_len // 100\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">With option to reduce the size of the dictionary:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:45:36.879043Z",
     "start_time": "2018-11-12T21:45:36.844053Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.9'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._word_stats = defaultdict(int)\n",
    "    \n",
    "    def filtervocab(self,top_n = 1000):\n",
    "        model_vocab = model._vocab\n",
    "        model_stat = model._word_stats\n",
    "        inds = [i[0] for i in sorted(self._word_stats.iteritems(),key=lambda (k): k[1],reverse=True)[:top_n]]\n",
    "        keys = []\n",
    "        newdict = {}\n",
    "        for key,item in (model_vocab.items()):\n",
    "            if item in inds:\n",
    "                newdict.update({key:item})\n",
    "        self._vocab = newdict\n",
    "        \n",
    "    def update_vocab(self, words_list):\n",
    "        for word in words_list:\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "            self._word_stats[self._vocab[word]] += 1\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        sample_loss = 0\n",
    "        predicted_tags = None\n",
    "\n",
    "        for tag in self._tags:\n",
    "            y = int(tag in tags)\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "            z = self._b[tag]\n",
    "\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                if self.freeze_vocab == True and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "                z += self._w[tag][self._vocab[word]]\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            sigma = sigmoid(z)\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "            sample_loss -= y*np.log(max(sigma,self.tolerance)) + (1 - y) * np.log(1 - min(sigma,1-self.tolerance))\n",
    "\n",
    "            if sample_id < self.top_n_train:\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "                dLdw = y - sigma\n",
    "\n",
    "                delta = self.learning_rate*dLdw\n",
    "                for word in question:\n",
    "                    if word in self._vocab:\n",
    "                    # HERE'S YOUR CODE\n",
    "                    # self._w[tag][self._vocab[word]] -= (- delta...\n",
    "                        weight = self._w[tag][self._vocab[word]]\n",
    "                        regularization = self.learning_rate * self.lambda_ * (2 * self.gamma * \n",
    "                                                  weight + (1 - self.gamma) * np.sign(weight))\n",
    "                        self._w[tag][self._vocab[word]] -= (-delta + regularization)\n",
    "                self._b[tag] -= -delta\n",
    "            else:\n",
    "                if predicted_tags is None:\n",
    "                    predicted_tags = []\n",
    "                # HERE'S YOUR CODE\n",
    "                # if sigma... :\n",
    "                if sigma > self.accuracy_level:\n",
    "                    predicted_tags.append(tag)\n",
    "                #     predicted_tags...\n",
    "\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        if predicted_tags is not None:\n",
    "            # HERE'S YOUR CODE\n",
    "            # Jaccard = ...\n",
    "            Jaccard = float(len(set(tags).intersection(set(predicted_tags))))/(len(tags) + len(predicted_tags) - \n",
    "                                                                     len(set(tags).intersection(set(predicted_tags))))\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': Jaccard})\n",
    "        else:\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': np.NaN})\n",
    "\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16,\n",
    "                      accuracy_level=0.9,\n",
    "                      lambda_=0.001,\n",
    "                      gamma = 0.1,\n",
    "                      freeze_vocab = False):\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.accuracy_level = accuracy_level\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.freeze_vocab = freeze_vocab\n",
    "\n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        if(freeze_vocab == False):\n",
    "            self.generate_vocab(df, column_name='question')\n",
    "        self.show_period = self.total_len // 100\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics\n",
    "    # HERE'S YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
